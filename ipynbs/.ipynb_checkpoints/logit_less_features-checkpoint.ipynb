{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for manipulating directory paths\n",
    "import os\n",
    "\n",
    "# Scientific and vector computation for python\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "# SciKit-Learn\n",
    "import sklearn as skl\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# Plotting library\n",
    "from matplotlib import pyplot\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Optimization module in scipy\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.zeros((7868, 76))\n",
    "Y_train = np.zeros((7868, 147))\n",
    "X_test = np.zeros((2705, 76))\n",
    "predictions = np.zeros((2705, 147))\n",
    "\n",
    "# Load X_train\n",
    "dir_string = '../data/train_feature_files'\n",
    "training_files = os.listdir(dir_string)\n",
    "\n",
    "for file in training_files:\n",
    "    file_path = dir_string + '/' + file\n",
    "    song_id_str = file_path.split('/')[-1].split('.')[0]\n",
    "    a = np.load(file_path)\n",
    "    a_summary = a['summary']\n",
    "    X_train[int(song_id_str)] = a_summary\n",
    "    \n",
    "# Load Y_train\n",
    "with open('../data/cal10k_train_data.csv', 'r') as y_train_file:\n",
    "    for line in y_train_file:\n",
    "        if 'id' not in line:\n",
    "            features_w_id = line.strip().split(',')\n",
    "            features_no_id = features_w_id[1:]\n",
    "            for idx in range(len(features_no_id)):\n",
    "                features_no_id[idx] = int(features_no_id[idx])\n",
    "            Y_train[int(features_w_id[0])] = np.array(features_no_id)\n",
    "            \n",
    "# Load X_test\n",
    "dir_string = '../data/test_feature_files/'\n",
    "testing_files = os.listdir(dir_string)\n",
    "\n",
    "for file in testing_files:\n",
    "    file_path = dir_string + file\n",
    "    song_id_str = file_path.split('/')[-1].split('.')[0]\n",
    "    data = np.load(file_path)\n",
    "    data_summary = data['summary']\n",
    "    X_test[int(song_id_str)] = data_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "where_are_NaNs = np.isnan(X_train)\n",
    "X_train[where_are_NaNs] = 0\n",
    "where_are_NaNs = np.isnan(X_test)\n",
    "X_test[where_are_NaNs] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled_X_train = preprocessing.scale(X_train)\n",
    "# scaled_X_test = preprocessing.scale(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction (by Hand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_indices = list()\n",
    "feature_indices.append(1)\n",
    "for i in range(3,15):\n",
    "    feature_indices.append(i)\n",
    "for i in range(27,40):\n",
    "    feature_indices.append(i)\n",
    "feature_indices.append(53)\n",
    "for i in range(56,63):\n",
    "    feature_indices.append(i)\n",
    "feature_indices.append(70)\n",
    "feature_indices.append(73)\n",
    "X_train_tempo_chroma_mfcc = X_train[:, feature_indices]\n",
    "X_test_tempo_chroma_mfcc = X_test[:, feature_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train that Shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147\n"
     ]
    }
   ],
   "source": [
    "genre_models = list()\n",
    "for i in range(147):  # For each genre\n",
    "    genre_model = LogisticRegression(solver='liblinear')\n",
    "    genre_model.fit(X_train_tempo_chroma_mfcc, Y_train[:,i])\n",
    "    genre_models.append(genre_model)\n",
    "    \n",
    "print(len(genre_models))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Some Things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  7.44797618e-03   4.50197360e-03   4.26336144e-02 ...,   2.03733491e-02\n",
      "    5.63474908e-03   1.20326136e-03]\n",
      " [  1.87900928e-03   1.43071509e-05   1.07504938e-02 ...,   6.78784935e-04\n",
      "    2.49255810e-04   1.03580289e-02]\n",
      " [  2.53776819e-03   4.98069435e-04   3.81272026e-02 ...,   2.85375914e-03\n",
      "    2.49741263e-03   4.08414996e-03]\n",
      " ..., \n",
      " [  5.86940679e-03   9.76081769e-03   2.38020817e-02 ...,   6.96657491e-03\n",
      "    2.45944980e-03   3.72161250e-02]\n",
      " [  4.07595004e-02   2.61308736e-02   2.18998357e-02 ...,   2.21267362e-02\n",
      "    4.28870188e-03   1.15018536e-03]\n",
      " [  5.81545199e-03   5.15934896e-04   3.83228284e-02 ...,   3.07788717e-03\n",
      "    2.15716867e-03   5.75399530e-02]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(147):\n",
    "    genre_prediction_prob = genre_models[i].predict_proba(X_test_tempo_chroma_mfcc)\n",
    "    predictions[:, i] = genre_prediction_prob[:,1]  # Get probability that it is class=1\n",
    "        \n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to a File with a Great Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2705, 147)\n"
     ]
    }
   ],
   "source": [
    "print(predictions.shape)\n",
    "file = open('../data/cal10k_test_random_submission.csv', 'r')\n",
    "id_line = None\n",
    "for line in file:\n",
    "    if 'id' in line:\n",
    "        id_line = line\n",
    "ids = []\n",
    "for i in range(2705):\n",
    "    ids.append(int(i))\n",
    "predictions = np.insert(predictions, 0, ids, axis=1)\n",
    "string_predictions = []\n",
    "for song_idx in range(len(predictions)):\n",
    "    song = predictions[song_idx]\n",
    "    song_string = []\n",
    "    for genre_idx in range(len(song)):\n",
    "        if genre_idx != 0:\n",
    "            song_string.append(str(predictions[song_idx, genre_idx]))\n",
    "        else: \n",
    "            song_string.append(str(predictions[song_idx, genre_idx]))\n",
    "    string_predictions.append(song_string)\n",
    "with open('../data/fuck_some_of_those_columns_tempo_chroma_mfcc_are_cool_tho.csv', 'w') as out_file:\n",
    "    out_file.write(id_line)\n",
    "    for song_array in string_predictions:\n",
    "        song_array[0] = str(int(float(song_array[0])))\n",
    "        out_file.write(\",\".join(song_array) + os.linesep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating this shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00131303  0.00038539  0.01094618 ...,  0.00113878  0.00152891\n",
      "   0.00449887]\n",
      " [ 0.00099141  0.0016229   0.04806449 ...,  0.0009814   0.00200187\n",
      "   0.05501445]\n",
      " [ 0.00236326  0.00483812  0.03486454 ...,  0.00609434  0.00733406\n",
      "   0.00367785]\n",
      " ..., \n",
      " [ 0.00177691  0.0003273   0.03175369 ...,  0.00128652  0.00465786\n",
      "   0.0425281 ]\n",
      " [ 0.00073046  0.00037186  0.01049603 ...,  0.00235111  0.00165572\n",
      "   0.12425217]\n",
      " [ 0.0021762   0.00106096  0.03207204 ...,  0.00131827  0.00697672\n",
      "   0.00972884]]\n"
     ]
    }
   ],
   "source": [
    "X_cv_train = X_train_tempo_chroma_mfcc[:-1500]\n",
    "Y_cv_train = Y_train[:-1500]\n",
    "X_cv_test = X_train_tempo_chroma_mfcc[-1500:]\n",
    "Y_cv_test = Y_train[-1500:]\n",
    "cv_predictions = np.zeros(Y_cv_test.shape)\n",
    "\n",
    "genre_models = list()\n",
    "for i in range(147):  # For each genre\n",
    "    genre_model = LogisticRegression(solver='liblinear')\n",
    "    genre_model.fit(X_cv_train, Y_cv_train[:,i])\n",
    "    genre_models.append(genre_model)\n",
    "\n",
    "\n",
    "for i in range(147):\n",
    "    genre_prediction_prob = genre_models[i].predict_proba(X_cv_test)\n",
    "    cv_predictions[:, i] = genre_prediction_prob[:,1]  # Get probability that it is class=1\n",
    "        \n",
    "print(cv_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Only one class present in y_true. ROC AUC score is not defined in that case.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-6e57e494d127>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_cv_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/machine_learning/lib/python3.6/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr)\u001b[0m\n\u001b[1;32m    354\u001b[0m     return _average_binary_score(\n\u001b[1;32m    355\u001b[0m         \u001b[0m_binary_roc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/machine_learning/lib/python3.6/site-packages/sklearn/metrics/base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0my_score_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnot_average_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         score[c] = binary_metric(y_true_c, y_score_c,\n\u001b[0;32m--> 120\u001b[0;31m                                  sample_weight=score_weight)\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;31m# Average the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/machine_learning/lib/python3.6/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36m_binary_roc_auc_score\u001b[0;34m(y_true, y_score, sample_weight)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_binary_roc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             raise ValueError(\"Only one class present in y_true. ROC AUC score \"\n\u001b[0m\u001b[1;32m    325\u001b[0m                              \"is not defined in that case.\")\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Only one class present in y_true. ROC AUC score is not defined in that case."
     ]
    }
   ],
   "source": [
    "print(skl.metrics.roc_auc_score(Y_cv_test, cv_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
